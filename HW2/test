import numpy as np
from scipy.special import softmax
from tqdm import tqdm
import matplotlib.pyplot as plt
from scipy.special import logsumexp

def log_smax(X, W):
        return X.dot(W.T) - np.expand_dims(logsumexp(X.dot(W.T), axis=1), 1)
    
# Please implement the fit(), predict(), and visualize_loss() methods of this
# class. You can add additional private methods by beginning them with two
# underscores. It may look like the __dummyPrivateMethod below. You can feel
# free to change any of the class attributes, as long as you do not change any
# of the given function headers (they must take and return the same arguments).

class LogisticRegression:
    def __init__(self, eta, lam):
        self.eta = eta
        self.lam = lam
        self.losses = []

    # Just to show how to make 'private' methods
    def __dummyPrivateMethod(self, input):
        return None
    
    # TODO: Implement this method!
    def fit(self, X, y):
        X = np.c_[X, np.ones(X.shape[0])]
        y_ohe = np.zeros((y.size, y.max()+1))
        y_ohe[np.arange(y.size), y] = 1
        y = y_ohe

        self.W = np.random.rand(3, X.shape[1])
        for i in tqdm(range(200000)):
            smax = np.exp(log_smax(X, self.W))
            grad = (smax-y).T @ X + self.lam * 2 * self.W
            self.W -= self.eta * grad
            loss = -np.sum(y*np.log(np.exp(log_smax(X, self.W)))) + \
                self.lam * (self.W ** 2).sum()
            self.losses.append(loss)
            
    # TODO: Implement this method!
    def predict(self, X_pred):
        # # The code in this method should be removed and replaced! We included it
        # # just so that the distribution code is runnable and produces a
        # # (currently meaningless) visualization.
        # preds = []
        # for x in X_pred:
        #     z = np.cos(x ** 2).sum()
        #     preds.append(1 + np.sign(z) * (np.abs(z) > 0.3))
        # return np.array(preds)
        X_pred = np.c_[X_pred, np.ones(X_pred.shape[0])]
        return np.argmax(X_pred @ self.W.T, axis=1)

    # TODO: Implement this method!
    def visualize_loss(self, output_file, show_charts=False):
        plt.plot(self.losses)
        plt.xlabel('Number of Iterations')
        plt.ylabel('Negative Log-Likelihood Loss')
        plt.title(output_file)
        plt.savefig(output_file)
        if show_charts:
            plt.show()
